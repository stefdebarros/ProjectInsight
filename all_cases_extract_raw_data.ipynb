{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID19 all cases in Brazil\n",
    "\n",
    "Here, publicly available data for all cases with flu like disease in Brazil who went through the healthcare system is extracted from an Elastic API.\n",
    "\n",
    "> **Data source**: all the data has been taken from the Brazilian Ministry of Health https://opendatasus.saude.gov.br/organization/ministerio-da-saude\n",
    ">\n",
    ">It requires translation from Portuguese to English\n",
    "\n",
    "This script is modified compared to the version I used on Azure: this version can be run on a \"standard\" local computer, the data is splitted 1.000.000 cases by parquet file, the data also being splitted by federal state. \n",
    "\n",
    "The aim is to spare memory ressources and disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch, exceptions, RequestsHttpConnection\n",
    "import elasticsearch.helpers\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from elasticsearch import ConnectionTimeout\n",
    "from urllib.error import HTTPError\n",
    "from whdh_py import DataLakeClient\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "import subprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Input parameters #####\n",
    "###List of abreviation designating Federative units of Brazil\n",
    "federative_units = ['ac','al','ap','am','ba','ce','df','es','go','ma','mt','ms','mg','pa','pb','pr','pe','pi','rj','rn',\n",
    "                    'rs','ro','rr','sc','to','se','sp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### API call\n",
    "es = Elasticsearch(hosts =[\"https://elasticsearch-saps.saude.gov.br\"],\n",
    "                   http_auth=('user-public-notificacoes','Za4qNXdyQNSa9YaA'),\n",
    "                  connection_class=RequestsHttpConnection,use_ssl = True,verify_certs = True,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####  Function to download non severe cases  #####\n",
    "def download_nonsevere_cases(federative_units):\n",
    "    ind_fu = 0\n",
    "    while ind_fu < len(federative_units):\n",
    "        fu = federative_units[ind_fu]\n",
    "#query definition\n",
    "        query= {\"query\":{\"bool\":{\"should\":[{\"exists\": {\"field\":\"resultadoTeste\"}},\n",
    "                                       {\"exists\": {\"field\":\"testes\"}},\n",
    "                                       {\"exists\":{\"field\":\"classificacaoFinal\"}}]}}}    \n",
    "# Initialize the scroll\n",
    "        try:\n",
    "            page = es.search(index=\"desc-esus-notifica-estado-\"+fu\n",
    "                         ,http_auth=('user-public-notificacoes','Za4qNXdyQNSa9YaA')\n",
    "                         ,scroll = '20m'\n",
    "                         ,timeout='50m'\n",
    "                         ,size = 10000\n",
    "                         ,body=query\n",
    "                        )\n",
    "            sid = page['_scroll_id']\n",
    "            scroll_size = page['hits']['total']['value']\n",
    "            dump_list = page['hits']['hits']\n",
    "            nber_part = 1\n",
    "            nber_scroll = 2\n",
    "        except (ConnectionTimeout):\n",
    "            nber_scroll = 1\n",
    "# Start scrolling\n",
    "        if nber_scroll == 2:\n",
    "            while (scroll_size > 0):\n",
    "                try:\n",
    "                    page = es.scroll(scroll_id = sid, scroll = '20m',request_timeout=50,\n",
    "                                 http_auth=('user-public-notificacoes','Za4qNXdyQNSa9YaA'))\n",
    "                    nber_scroll = nber_scroll+1\n",
    "                    # Update the scroll ID\n",
    "                    sid = page['_scroll_id']\n",
    "                    # Get the number of results that we returned in the last scroll\n",
    "                    scroll_size = len(page['hits']['hits'])\n",
    "                    # Save data in parquet file\n",
    "                    if scroll_size > 0:\n",
    "                        dump_list.extend(page['hits']['hits'])\n",
    "                        if len(dump_list) == 1000000:\n",
    "                            df_data = pd.DataFrame(dump_list)\n",
    "                            dump_list.clear()\n",
    "                            df_data = df_data.drop(columns=['_index','_type','_id','_score'])\n",
    "                            try:\n",
    "                                df_output = pd.json_normalize(df_data['_source'])\n",
    "                            except (AttributeError):\n",
    "                                df_data['_source'] = df_data['_source'].apply(lambda x:eval(x))\n",
    "                                df_output = pd.json_normalize(df_data['_source'])\n",
    "                            del df_data\n",
    "                            local_path='extracted_data_'+fu+'_part'+str(nber_part)+'.parquet'\n",
    "                            df_output.to_parquet(local_path)\n",
    "                            del df_output\n",
    "                            nber_part += 1\n",
    "                except (ConnectionTimeout):\n",
    "                    nber_scroll = 1\n",
    "                    scroll_size = 0\n",
    "            if nber_scroll != 1:\n",
    "                df_data = pd.DataFrame(dump_list)\n",
    "                df_data = df_data.drop(columns=['_index','_type','_id','_score'])\n",
    "                try:\n",
    "                    df_output = pd.json_normalize(df_data['_source'])\n",
    "                except (AttributeError):\n",
    "                    df_data['_source'] = df_data['_source'].apply(lambda x:eval(x))\n",
    "                    df_output = pd.json_normalize(df_data['_source'])\n",
    "                del df_data\n",
    "                local_path='extracted_data_'+fu+'_part'+str(nber_part)+'.parquet'\n",
    "                df_output.to_parquet(local_path)\n",
    "                del df_output\n",
    "                ind_fu += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####  Download non severe cases  #####\n",
    "if __name__ == '__main__':\n",
    "    ncores = mp.cpu_count()\n",
    "    # multiprocessing pool object\n",
    "    pool = mp.Pool(ncores)  \n",
    "    # input list\n",
    "    inputs = [federative_units]\n",
    "    # map the function to the list and pass\n",
    "    # function and input list as arguments\n",
    "    pool.map(download_nonsevere_cases,inputs)\n",
    "    pool.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
